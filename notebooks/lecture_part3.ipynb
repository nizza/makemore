{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb35708-6b76-49bd-be76-3b413da1994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d35096-c832-443f-abce-7f0504fa53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17270b7c-9f05-456d-9e2a-d8b2cb9f2ddf",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0e7fa7-8692-49fc-9127-cd05d4ec3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4126f02c-1daf-406a-8153-54695c993ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the names from the file\n",
    "names = open(file_path, 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b485bbac-780e-4939-bfcd-0a33bf762a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating vocabulary of all possible characters\n",
    "vocabulary = sorted(list(set([c for word in names for c in word])))\n",
    "vocabulary = ['.'] + vocabulary  # adding start/end character\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e796d267-a6fa-4ad6-8cca-adfe63ebcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionaries to convert from character to numerical index, and viceversa\n",
    "stoi = {v:i for i,v in enumerate(vocabulary)}\n",
    "itos = {i:v for i,v in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ee469b-ba5e-412c-b408-49a12ab4e7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3204)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting data in train - val - test\n",
    "random.shuffle(names)\n",
    "\n",
    "end_train = int(len(names) * 0.8)\n",
    "end_val = int(len(names) * 0.9)\n",
    "names_train = names[:end_train]\n",
    "names_val = names[end_train:end_val]\n",
    "names_test = names[end_val:]\n",
    "len(names_train), len(names_val), len(names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2768a65f-3d95-44e3-8ea3-fcbfc7d91f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "\n",
    "def make_data(names, context_size = 3):\n",
    "\n",
    "    X, y  = [], []\n",
    "    \n",
    "    for name in names[:]:\n",
    "    \n",
    "        # initializing context (empty) to predict the beginning of the name\n",
    "        x_i = [stoi['.']] * context_size\n",
    "    \n",
    "        # going through the characters in the name\n",
    "        for c in name + '.':\n",
    "    \n",
    "            # we try to predict the next character given the context\n",
    "            y_i = stoi[c]\n",
    "            X.append(x_i)\n",
    "            y.append(y_i)\n",
    "    \n",
    "            # sliding the context \n",
    "            x_i = x_i[1:] + [y_i] \n",
    "    \n",
    "    # converting to tensors\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adceecae-4af8-42aa-9d21-1724d27872a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182484, 5]) torch.Size([182484])\n",
      "torch.Size([22850, 5]) torch.Size([22850])\n",
      "torch.Size([22812, 5]) torch.Size([22812])\n"
     ]
    }
   ],
   "source": [
    "context_size = 5\n",
    "X_train, y_train = make_data(names_train, context_size)\n",
    "X_val, y_val = make_data(names_val, context_size)\n",
    "X_test, y_test = make_data(names_test, context_size)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e99584-db1f-4be1-bf0b-935347eaed1f",
   "metadata": {},
   "source": [
    "# Creating  the models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3aa893-294e-4946-b446-8adffa7e8add",
   "metadata": {},
   "source": [
    "## Improving the weights initialization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e21cfbe-6926-4376-8ff3-7535e84e2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model_v1():\n",
    "\n",
    "    def __init__(self, emb_size=2, vocabulary_size=27, context_size = 3, layer1_size=100,\n",
    "                 W1_std=1, B1_std=1, W2_std=1, B2_std=1):\n",
    "        # Input layer\n",
    "        # the index of the character is used to retrieve an  embedding vector of size @\n",
    "        self.emb_size = emb_size\n",
    "        self.C  = torch.randn((vocabulary_size, emb_size), requires_grad=True)\n",
    "        # First fully connected layer with 100 neurons\n",
    "        self.context_size = context_size\n",
    "        self.input_size = context_size * emb_size\n",
    "        self.layer1_size = layer1_size\n",
    "        self.W1 = (torch.randn((self.input_size, layer1_size)) * W1_std).clone().detach().requires_grad_(True)\n",
    "        self.B1 = (torch.randn(layer1_size) * B1_std ).clone().detach().requires_grad_(True)\n",
    "        # Last layer with 27 neurons (the size of the vocabulary)\n",
    "        self.W2 = (torch.randn((layer1_size, vocabulary_size))* W2_std).clone().detach().requires_grad_(True)\n",
    "        self.B2 = (torch.randn(vocabulary_size) * B2_std).clone().detach().requires_grad_(True)\n",
    "        # Creating list with all  trainable params\n",
    "        self.params = [self.C, self.W1, self.B1, self.W2, self.B2]\n",
    "\n",
    "        # initializing loss\n",
    "        self.loss = None\n",
    "        \n",
    "\n",
    "    def __call__(self, X):\n",
    "        # Getting the input embedding\n",
    "        X_emb = self.C[X]\n",
    "        # Flattening X_emb so that each context is represented as one dimensional  vector\n",
    "        X_flat = X_emb.view(-1, self.input_size)\n",
    "        # Applying first layer\n",
    "        X1 = (X_flat @ self.W1 + self.B1).tanh()\n",
    "        # Applying last layer\n",
    "        X2 = (X1 @ self.W2 + self.B2)\n",
    "        # Normalizing output (softmax)\n",
    "        #counts = X2.exp()\n",
    "        #probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return X2\n",
    "\n",
    "    def forward_pass(self, X, y):\n",
    "        # computing model output\n",
    "        #probs =  self.__call__(X)\n",
    "        logits = self.__call__(X)\n",
    "        \n",
    "        # Normalizing output (softmax)\n",
    "        #counts = X2.exp()\n",
    "        #probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # getting, for each row of probs, the value corresponding to the actual next character in the training set\n",
    "        #preds = probs[range(len(y)), y]\n",
    "        \n",
    "        # computing the average, of the negative logs\n",
    "        #self.loss = -preds.log().mean()\n",
    "\n",
    "        # computing the loss, efficient way\n",
    "        self.loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss.item()\n",
    "\n",
    "    def backward_pass(self):\n",
    "\n",
    "        # skipping if loss has never been computed\n",
    "        if self.loss is None:\n",
    "            return\n",
    "\n",
    "        # Resetting the gradients\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "        \n",
    "        # performing the backpropagation of the loss to compute the gradients\n",
    "        self.loss.backward()\n",
    "\n",
    "        # Updating the params\n",
    "        alpha = 0.1\n",
    "        for p in self.params:\n",
    "            p.data += - alpha * p.grad\n",
    "        \n",
    "    def predict_probs(self, X):\n",
    "\n",
    "        # getting logits\n",
    "        logits = self.__call__(X)\n",
    "\n",
    "        # Normalizing output (softmax)\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Getting the indexes with highest probabilities\n",
    "        #preds = probs.argmax(axis=1)\n",
    "        \n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea6fd55-1a57-49c2-b8dc-782baeec4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, loss:16.517805099487305\n",
      "i: 10000, loss:2.459197521209717\n",
      "i: 20000, loss:2.457740306854248\n",
      "i: 30000, loss:2.487136125564575\n",
      "i: 40000, loss:2.3575973510742188\n",
      "i: 50000, loss:2.1241016387939453\n",
      "i: 60000, loss:2.01373028755188\n",
      "i: 70000, loss:2.295576810836792\n",
      "\n",
      "validation dataset loss:2.2350573539733887\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "m = Model_v1(emb_size=8, context_size=context_size)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(80000):\n",
    "\n",
    "    # generating the indexes of the elements to be used in the batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    \n",
    "    m.forward_pass(X_train[batch_idx], y_train[batch_idx])\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"i: {i}, loss:{m.get_loss()}\")\n",
    "    #print(m.get_loss())\n",
    "    \n",
    "    m.backward_pass()\n",
    "\n",
    "# Computing the loss on the whole validation dataset\n",
    "m.forward_pass(X_val, y_val)\n",
    "print()\n",
    "print(f\"validation dataset loss:{m.get_loss()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0306da8-546a-4700-ab2c-35b2e87437a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, loss:4.434738636016846\n",
      "i: 10000, loss:2.542330026626587\n",
      "i: 20000, loss:2.2687408924102783\n",
      "i: 30000, loss:2.300877094268799\n",
      "i: 40000, loss:2.0275070667266846\n",
      "i: 50000, loss:2.502208948135376\n",
      "i: 60000, loss:2.4851906299591064\n",
      "i: 70000, loss:2.295337200164795\n",
      "\n",
      "validation dataset loss:2.1837849617004395\n"
     ]
    }
   ],
   "source": [
    "# Training the model with a better initialization of the last layer\n",
    "# We modify the std of W2 and B2 so that we start with a flat output distribution\n",
    "# to diminish the risk of the model being overly confident on some wrong predictions\n",
    "\n",
    "m = Model_v1(emb_size=8, context_size=context_size, W2_std=0.2, B2_std=0)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(80000):\n",
    "\n",
    "    # generating the indexes of the elements to be used in the batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    \n",
    "    m.forward_pass(X_train[batch_idx], y_train[batch_idx])\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"i: {i}, loss:{m.get_loss()}\")\n",
    "    #print(m.get_loss())\n",
    "    \n",
    "    m.backward_pass()\n",
    "\n",
    "# Computing the loss on the whole validation dataset\n",
    "m.forward_pass(X_val, y_val)\n",
    "print()\n",
    "print(f\"validation dataset loss:{m.get_loss()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad92398-bf0c-4faf-ae2b-82f5576520ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, loss:4.204208850860596\n",
      "i: 10000, loss:2.41817307472229\n",
      "i: 20000, loss:2.145571231842041\n",
      "i: 30000, loss:2.2393765449523926\n",
      "i: 40000, loss:2.1258485317230225\n",
      "i: 50000, loss:2.3216745853424072\n",
      "i: 60000, loss:2.2246437072753906\n",
      "i: 70000, loss:1.9209933280944824\n",
      "\n",
      "validation dataset loss:2.1187126636505127\n"
     ]
    }
   ],
   "source": [
    "# Training the model with a better initialization of the first layer\n",
    "# We modify the std of W1 and B1 so that the distribution of the output of the layer\n",
    "# stays around 0:\n",
    "#   - around 0, the gradient of the tanh function is linear and can backpropagate gradient effectively\n",
    "#   - for values far from 0 the gradient of tanh will be almost so, it will prevent information from\n",
    "#     the later layer to be backpropagated effectively (g * 0 = 0)\n",
    "\n",
    "\n",
    "# We use the kaiming rule to initialize W1\n",
    "# a = gain / sqrt(fan_in)\n",
    "# gain constant that depends on the activation (2/3 for tanh)\n",
    "# fan_in is the the number of input to the layer (emb_size * context_size in our case)\n",
    "emb_size=8\n",
    "fan_in = emb_size * context_size\n",
    "W1_std = (3/2) / fan_in**0.5\n",
    "\n",
    "\n",
    "m = Model_v1(emb_size=emb_size, context_size=context_size, W1_std=W1_std, B1_std=0.1, W2_std=0.2, B2_std=0)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(80000):\n",
    "\n",
    "    # generating the indexes of the elements to be used in the batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    \n",
    "    m.forward_pass(X_train[batch_idx], y_train[batch_idx])\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"i: {i}, loss:{m.get_loss()}\")\n",
    "    #print(m.get_loss())\n",
    "    \n",
    "    m.backward_pass()\n",
    "\n",
    "# Computing the loss on the whole validation dataset\n",
    "m.forward_pass(X_val, y_val)\n",
    "print()\n",
    "print(f\"validation dataset loss:{m.get_loss()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631760d1-6a89-4b87-8ad4-1ee0b2703ba1",
   "metadata": {},
   "source": [
    "## Adding batch normalization to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c38caf0-4b41-4773-a090-5cc67a6ca619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model_v2():\n",
    "\n",
    "    def __init__(self, emb_size=2, vocabulary_size=27, context_size = 3, layer1_size=100,\n",
    "                 W1_std=1, B1_std=1, W2_std=1, B2_std=1):\n",
    "        # Input layer\n",
    "        # the index of the character is used to retrieve an  embedding vector of size @\n",
    "        self.emb_size = emb_size\n",
    "        self.C  = torch.randn((vocabulary_size, emb_size), requires_grad=True)\n",
    "        # First fully connected layer with 100 neurons\n",
    "        self.context_size = context_size\n",
    "        self.input_size = context_size * emb_size\n",
    "        self.layer1_size = layer1_size\n",
    "        self.W1 = (torch.randn((self.input_size, layer1_size)) * W1_std).clone().detach().requires_grad_(True)\n",
    "        self.B1 = (torch.randn(layer1_size) * B1_std ).clone().detach().requires_grad_(True)\n",
    "        # Batch normalization layer, to apply before the non linearity\n",
    "        self.bn_gain = torch.randn(layer1_size, requires_grad=True)\n",
    "        self.bn_bias = torch.randn(layer1_size, requires_grad=True)\n",
    "        self.bn_mean = torch.randn(layer1_size,)\n",
    "        self.bn_std = torch.randn(layer1_size,)\n",
    "        # Last layer with 27 neurons (the size of the vocabulary)\n",
    "        self.W2 = (torch.randn((layer1_size, vocabulary_size))* W2_std).clone().detach().requires_grad_(True)\n",
    "        self.B2 = (torch.randn(vocabulary_size) * B2_std).clone().detach().requires_grad_(True)\n",
    "        # Creating list with all  trainable params\n",
    "        self.params = [self.C, self.W1, self.B1, self.bn_gain, self.bn_bias, self.W2, self.B2]\n",
    "\n",
    "        # initializing loss\n",
    "        self.loss = None\n",
    "        \n",
    "\n",
    "    def __call__(self, X):\n",
    "        # Getting the input embedding\n",
    "        X_emb = self.C[X]\n",
    "        # Flattening X_emb so that each context is represented as one dimensional  vector\n",
    "        X_flat = X_emb.view(-1, self.input_size)\n",
    "        # Applying first layer\n",
    "        X1 = (X_flat @ self.W1 + self.B1) # linear\n",
    "        with torch.no_grad():\n",
    "            # bn bias are updated manually, not through backpropagation\n",
    "            self.bn_mean  = 0.99 * self.bn_mean +  0.01 * X1.mean(axis=0, keepdims=True)\n",
    "            self.bn_std  = 0.99 * self.bn_std +  0.01 * X1.std(axis=0, keepdims=True, correction=0)\n",
    "        # batch normalization , adding small  epsilon to avoid divide by zero when the batch variance is 0\n",
    "        X1 = (X1 - self.bn_mean) / (self.bn_std + 0.001) \n",
    "        X1 = (X1 * self.bn_gain) + self.bn_bias  # batch normalization gain and bias\n",
    "        X1 = X1.tanh() #non linearity\n",
    "        # Applying last layer\n",
    "        X2 = (X1 @ self.W2 + self.B2)\n",
    "        # Normalizing output (softmax)\n",
    "        #counts = X2.exp()\n",
    "        #probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return X2\n",
    "\n",
    "    def forward_pass(self, X, y):\n",
    "        # computing model output\n",
    "        #probs =  self.__call__(X)\n",
    "        logits = self.__call__(X)\n",
    "        \n",
    "        # Normalizing output (softmax)\n",
    "        #counts = X2.exp()\n",
    "        #probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # getting, for each row of probs, the value corresponding to the actual next character in the training set\n",
    "        #preds = probs[range(len(y)), y]\n",
    "        \n",
    "        # computing the average, of the negative logs\n",
    "        #self.loss = -preds.log().mean()\n",
    "\n",
    "        # computing the loss, efficient way\n",
    "        self.loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss.item()\n",
    "\n",
    "    def backward_pass(self):\n",
    "\n",
    "        # skipping if loss has never been computed\n",
    "        if self.loss is None:\n",
    "            return\n",
    "\n",
    "        # Resetting the gradients\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "        \n",
    "        # performing the backpropagation of the loss to compute the gradients\n",
    "        self.loss.backward()\n",
    "\n",
    "        # Updating the params\n",
    "        alpha = 0.1\n",
    "        for p in self.params:\n",
    "            p.data += - alpha * p.grad\n",
    "        \n",
    "    def predict_probs(self, X):\n",
    "\n",
    "        # getting logits\n",
    "        logits = self.__call__(X)\n",
    "\n",
    "        # Normalizing output (softmax)\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Getting the indexes with highest probabilities\n",
    "        #preds = probs.argmax(axis=1)\n",
    "        \n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec57b046-ca8d-4171-aba0-7f309c3551db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, loss:4.718467712402344\n",
      "i: 10000, loss:2.348910331726074\n",
      "i: 20000, loss:2.0643563270568848\n",
      "i: 30000, loss:2.02232027053833\n",
      "i: 40000, loss:1.967730164527893\n",
      "i: 50000, loss:2.201854944229126\n",
      "i: 60000, loss:2.1376073360443115\n",
      "i: 70000, loss:1.9560177326202393\n",
      "\n",
      "validation dataset loss:2.111684799194336\n"
     ]
    }
   ],
   "source": [
    "# Trainig the model with batch normalization\n",
    "m = Model_v2(emb_size=emb_size, context_size=context_size, W1_std=W1_std, B1_std=0.1, W2_std=0.2, B2_std=0)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(80000):\n",
    "\n",
    "    # generating the indexes of the elements to be used in the batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    \n",
    "    m.forward_pass(X_train[batch_idx], y_train[batch_idx])\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"i: {i}, loss:{m.get_loss()}\")\n",
    "    #print(m.get_loss())\n",
    "    \n",
    "    m.backward_pass()\n",
    "\n",
    "# Computing the loss on the whole validation dataset\n",
    "m.forward_pass(X_val, y_val)\n",
    "print()\n",
    "print(f\"validation dataset loss:{m.get_loss()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de3688-96d3-4d4c-9597-cdd86cd488d6",
   "metadata": {},
   "source": [
    "# Generating names from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb72464d-570c-4916-90e2-8b7bc856ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_name(model):\n",
    "    \n",
    "    # Initializing output sequence\n",
    "    result = []\n",
    "    \n",
    "    # Creating starting empty sequence compatible with context size of the model\n",
    "    context = torch.zeros([1, model.context_size]).int()\n",
    "    \n",
    "    # Generating until the end value (0) is returned\n",
    "    pred_value = -1\n",
    "    while pred_value != 0:\n",
    "    \n",
    "        # predicting probability distribution for next character\n",
    "        probs = model.predict_probs(context)\n",
    "        # getting a sample from the probability distribution\n",
    "        pred = torch.multinomial(probs, 1, replacement=True)\n",
    "        pred_value = pred.item()\n",
    "    \n",
    "        # updating the context with the new character\n",
    "        context = torch.concat((context[0, 1:], pred[0])).unsqueeze(0)\n",
    "    \n",
    "        # adding predicted character to the output sequence\n",
    "        result.append(itos[pred_value])\n",
    "    \n",
    "    return ''.join(result[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff7f445a-7b60-4277-aaa6-7c008ff9e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaley\n",
      "millee\n",
      "magymin\n",
      "thayloughan\n",
      "trayieh\n",
      "mylah\n",
      "yahj\n",
      "reyvon\n",
      "showyn\n",
      "jovingell\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate_name(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ae68c-5a7c-4870-b2a0-2bca30a1d937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
