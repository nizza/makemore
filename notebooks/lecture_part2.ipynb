{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb35708-6b76-49bd-be76-3b413da1994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d35096-c832-443f-abce-7f0504fa53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17270b7c-9f05-456d-9e2a-d8b2cb9f2ddf",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0e7fa7-8692-49fc-9127-cd05d4ec3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4126f02c-1daf-406a-8153-54695c993ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the names from the file\n",
    "names = open(file_path, 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b485bbac-780e-4939-bfcd-0a33bf762a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating vocabulary of all possible characters\n",
    "vocabulary = sorted(list(set([c for word in names for c in word])))\n",
    "vocabulary = ['.'] + vocabulary  # adding start/end character\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e796d267-a6fa-4ad6-8cca-adfe63ebcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionaries to convert from character to numerical index, and viceversa\n",
    "stoi = {v:i for i,v in enumerate(vocabulary)}\n",
    "itos = {i:v for i,v in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ee469b-ba5e-412c-b408-49a12ab4e7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3204)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting data in train - val - test\n",
    "random.shuffle(names)\n",
    "\n",
    "end_train = int(len(names) * 0.8)\n",
    "end_val = int(len(names) * 0.9)\n",
    "names_train = names[:end_train]\n",
    "names_val = names[end_train:end_val]\n",
    "names_test = names[end_val:]\n",
    "len(names_train), len(names_val), len(names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2768a65f-3d95-44e3-8ea3-fcbfc7d91f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182467, 3]) torch.Size([182467])\n",
      "torch.Size([22837, 3]) torch.Size([22837])\n",
      "torch.Size([22842, 3]) torch.Size([22842])\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset\n",
    "\n",
    "def make_data(names, context_size = 3):\n",
    "\n",
    "    X, y  = [], []\n",
    "    \n",
    "    for name in names[:]:\n",
    "    \n",
    "        # initializing context (empty) to predict the beginning of the name\n",
    "        x_i = [stoi['.']] * context_size\n",
    "    \n",
    "        # going through the characters in the name\n",
    "        for c in name + '.':\n",
    "    \n",
    "            # we try to predict the next character given the context\n",
    "            y_i = stoi[c]\n",
    "            X.append(x_i)\n",
    "            y.append(y_i)\n",
    "    \n",
    "            # sliding the context \n",
    "            x_i = x_i[1:] + [y_i] \n",
    "    \n",
    "    # converting to tensors\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = make_data(names_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_val, y_val = make_data(names_val)\n",
    "print(X_val.shape, y_val.shape)\n",
    "X_test, y_test = make_data(names_test)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e99584-db1f-4be1-bf0b-935347eaed1f",
   "metadata": {},
   "source": [
    "# Creating  a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032895ae-cef8-4fe8-b26e-93e61b0ae268",
   "metadata": {},
   "source": [
    "## Prototyping a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbbebfb1-dfd7-4534-b6a5-e759d99994b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list to store all the trainable parameters\n",
    "params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50431dcf-50f6-414f-b7f3-d764a5f9c9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input layer\n",
    "# the index of the character is used to retrieve an  embedding vector of size @\n",
    "emb_size = 2\n",
    "C  = torch.randn((len(vocabulary), emb_size), requires_grad=True)\n",
    "params.append(C)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fab5fa1-73a0-4ca5-bf27-e984611fb83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182467, 3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the input embedding\n",
    "X_emb = C[X_train]\n",
    "X_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "870576f8-04fd-451b-af0e-5b805170ff9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182467, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flattening X_emb so that each context is represented as one dimensional  vector\n",
    "input_size = X_emb.shape[-1] *  X_emb.shape[-2] \n",
    "X_flat = X_emb.view(-1, input_size)\n",
    "X_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd26c60-a78b-4980-85fa-0be2d8922b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 100]), torch.Size([100]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First fully connected layer with 100 neurons\n",
    "W1 = torch.randn((input_size, 100), requires_grad=True)\n",
    "B1 = torch.randn((100), requires_grad=True)\n",
    "params.extend([W1, B1])\n",
    "W1.shape, B1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e8d9c5-2ee1-4423-954f-9d1ab683159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182467, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying first layer\n",
    "X1 = (X_flat @ W1 + B1).tanh()\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e604ae6-9cd2-4bf7-9ec9-0a2f876cb81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 27]), torch.Size([27]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last layer with 27 neurons (the size of the vocabulary)\n",
    "W2 = torch.randn((100, 27), requires_grad=True)\n",
    "B2 = torch.randn((27), requires_grad=True)\n",
    "params.extend([W2, B2])\n",
    "W2.shape, B2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b911201-1822-43a0-abf4-90a361ebcdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182467, 27])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying last layer\n",
    "X2 = (X1 @ W2 + B2).tanh()\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0020c3d-75a2-4465-ad9e-7234e93c5cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182467, 27])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing output (softmax)\n",
    "counts = X2.exp()\n",
    "probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "#probs.sum(axis=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbef2167-e448-46ce-bbf0-960903c28899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7313, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing the loss\n",
    "\n",
    "# getting, for each row of probs, the value corresponding to the actual next character in the training set\n",
    "preds = probs[range(len(y_train)), y_train]\n",
    "\n",
    "# computing the average, of the negative logs\n",
    "loss = -preds.log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b237c12-4c45-4e5f-8264-7f24f984f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing the backpropagation of the loss to compute the gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e678be5-cdf1-4ac7-8118-2c9f2b7032e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the params\n",
    "alpha = 0.01\n",
    "for p in params:\n",
    "    p = p - alpha * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3aa893-294e-4946-b446-8adffa7e8add",
   "metadata": {},
   "source": [
    "## Defining functions to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e21cfbe-6926-4376-8ff3-7535e84e2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, emb_size=2, vocabulary_size=27, context_size = 3, layer1_size=100):\n",
    "        # Input layer\n",
    "        # the index of the character is used to retrieve an  embedding vector of size @\n",
    "        self.emb_size = emb_size\n",
    "        self.C  = torch.randn((vocabulary_size, emb_size), requires_grad=True)\n",
    "        # First fully connected layer with 100 neurons\n",
    "        self.context_size = context_size\n",
    "        self.input_size = self.context_size * self.emb_size\n",
    "        self.layer1_size = layer1_size\n",
    "        self.W1 = torch.randn((self.input_size, self.layer1_size), requires_grad=True)\n",
    "        self.B1 = torch.randn((self.layer1_size), requires_grad=True)\n",
    "        # Last layer with 27 neurons (the size of the vocabulary)\n",
    "        self.W2 = torch.randn((self.layer1_size, vocabulary_size), requires_grad=True)\n",
    "        self.B2 = torch.randn((vocabulary_size), requires_grad=True)\n",
    "        # Creating list with all  trainable params\n",
    "        self.params = [self.C, self.W1, self.B1, self.W2, self.B2]\n",
    "\n",
    "        # initializing loss\n",
    "        self.loss = None\n",
    "        \n",
    "\n",
    "    def __call__(self, X):\n",
    "        # Getting the input embedding\n",
    "        X_emb = self.C[X]\n",
    "        # Flattening X_emb so that each context is represented as one dimensional  vector\n",
    "        X_flat = X_emb.view(-1, self.input_size)\n",
    "        # Applying first layer\n",
    "        X1 = (X_flat @ self.W1 + self.B1).tanh()\n",
    "        # Applying last layer\n",
    "        X2 = (X1 @ self.W2 + self.B2)\n",
    "        # Normalizing output (softmax)\n",
    "        #counts = X2.exp()\n",
    "        #probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return X2\n",
    "\n",
    "    def forward_pass(self, X, y):\n",
    "        # computing model output\n",
    "        #probs =  self.__call__(X)\n",
    "        logits = self.__call__(X)\n",
    "        \n",
    "        # Normalizing output (softmax)\n",
    "        #counts = X2.exp()\n",
    "        #probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # getting, for each row of probs, the value corresponding to the actual next character in the training set\n",
    "        #preds = probs[range(len(y)), y]\n",
    "        \n",
    "        # computing the average, of the negative logs\n",
    "        #self.loss = -preds.log().mean()\n",
    "\n",
    "        # computing the loss, efficient way\n",
    "        self.loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss.item()\n",
    "\n",
    "    def backward_pass(self):\n",
    "\n",
    "        # skipping if loss has never been computed\n",
    "        if self.loss is None:\n",
    "            print('yo')\n",
    "            return\n",
    "\n",
    "        # Resetting the gradients\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "        \n",
    "        # performing the backpropagation of the loss to compute the gradients\n",
    "        self.loss.backward()\n",
    "\n",
    "        # Updating the params\n",
    "        alpha = 0.1\n",
    "        for p in self.params:\n",
    "            p.data += - alpha * p.grad\n",
    "        \n",
    "    def predict_probs(self, X):\n",
    "\n",
    "        # getting logits\n",
    "        logits = self.__call__(X)\n",
    "\n",
    "        # Normalizing output (softmax)\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Getting the indexes with highest probabilities\n",
    "        #preds = probs.argmax(axis=1)\n",
    "        \n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e966fe4-6802-4bdd-bff1-f5d22bc4d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, loss:14.496805191040039\n",
      "i: 10, loss:9.772998809814453\n",
      "i: 20, loss:7.365072727203369\n",
      "i: 30, loss:5.94071102142334\n",
      "i: 40, loss:5.035120487213135\n",
      "i: 50, loss:4.48888635635376\n",
      "i: 60, loss:4.1285881996154785\n",
      "i: 70, loss:3.867497682571411\n",
      "i: 80, loss:3.6675920486450195\n",
      "i: 90, loss:3.508657693862915\n",
      "\n",
      "validation dataset loss:3.3483662605285645\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "m = Model()\n",
    "\n",
    "for i in range(100):\n",
    "    m.forward_pass(X_train, y_train)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"i: {i}, loss:{m.get_loss()}\")\n",
    "    #print(m.get_loss())\n",
    "    \n",
    "    m.backward_pass()\n",
    "\n",
    "# Computing the loss on the validation dataset\n",
    "m.forward_pass(X_val, y_val)\n",
    "print()\n",
    "print(f\"validation dataset loss:{m.get_loss()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c58a92f-15ed-43dc-92ac-ce1876e7bf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, loss:16.090801239013672\n",
      "i: 10000, loss:2.8609697818756104\n",
      "i: 20000, loss:2.5561227798461914\n",
      "i: 30000, loss:2.1848437786102295\n",
      "i: 40000, loss:2.2716543674468994\n",
      "i: 50000, loss:2.133680820465088\n",
      "\n",
      "validation dataset loss:2.3511996269226074\n"
     ]
    }
   ],
   "source": [
    "# Training the model, with batches, instead of whole dataset\n",
    "\n",
    "m = Model()\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(60000):\n",
    "\n",
    "    # generating the indexes of the elements to be used in the batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    \n",
    "    m.forward_pass(X_train[batch_idx], y_train[batch_idx])\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"i: {i}, loss:{m.get_loss()}\")\n",
    "    #print(m.get_loss())\n",
    "    \n",
    "    m.backward_pass()\n",
    "\n",
    "# Computing the loss on the validation dataset\n",
    "m.forward_pass(X_val, y_val)\n",
    "print()\n",
    "print(f\"validation dataset loss:{m.get_loss()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c75c6-3897-4667-8f74-6d684233a456",
   "metadata": {},
   "source": [
    "## Improving the architecture of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ea6fd55-1a57-49c2-b8dc-782baeec4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0, loss:17.27199363708496\n",
      "i: 10000, loss:2.8270864486694336\n",
      "i: 20000, loss:2.48972225189209\n",
      "i: 30000, loss:2.117682695388794\n",
      "i: 40000, loss:2.306112766265869\n",
      "i: 50000, loss:2.3946666717529297\n",
      "i: 60000, loss:2.2828826904296875\n",
      "i: 70000, loss:2.2651262283325195\n",
      "\n",
      "validation dataset loss:2.2664685249328613\n"
     ]
    }
   ],
   "source": [
    "# Training the model, with batches, instead of whole dataset\n",
    "\n",
    "context_size = 5\n",
    "X_train, y_train = make_data(names_train, context_size)\n",
    "X_val, y_val = make_data(names_val, context_size)\n",
    "X_test, y_test = make_data(names_test, context_size)\n",
    "\n",
    "\n",
    "m = Model(emb_size=8, context_size=context_size)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(80000):\n",
    "\n",
    "    # generating the indexes of the elements to be used in the batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    \n",
    "    m.forward_pass(X_train[batch_idx], y_train[batch_idx])\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"i: {i}, loss:{m.get_loss()}\")\n",
    "    #print(m.get_loss())\n",
    "    \n",
    "    m.backward_pass()\n",
    "\n",
    "# Computing the loss on the whole validation dataset\n",
    "m.forward_pass(X_val, y_val)\n",
    "print()\n",
    "print(f\"validation dataset loss:{m.get_loss()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de3688-96d3-4d4c-9597-cdd86cd488d6",
   "metadata": {},
   "source": [
    "# Generating names from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb72464d-570c-4916-90e2-8b7bc856ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_name(model):\n",
    "    \n",
    "    # Initializing output sequence\n",
    "    result = []\n",
    "    \n",
    "    # Creating starting empty sequence compatible with context size of the model\n",
    "    context = torch.zeros([1, model.context_size]).int()\n",
    "    \n",
    "    # Generating until the end value (0) is returned\n",
    "    pred_value = -1\n",
    "    while pred_value != 0:\n",
    "    \n",
    "        # predicting probability distribution for next character\n",
    "        probs = model.predict_probs(context)\n",
    "        # getting a sample from the probability distribution\n",
    "        pred = torch.multinomial(probs, 1, replacement=True)\n",
    "        pred_value = pred.item()\n",
    "    \n",
    "        # updating the context with the new character\n",
    "        context = torch.concat((context[0, 1:], pred[0])).unsqueeze(0)\n",
    "    \n",
    "        # adding predicted character to the output sequence\n",
    "        result.append(itos[pred_value])\n",
    "    \n",
    "    return ''.join(result[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff7f445a-7b60-4277-aaa6-7c008ff9e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlengya\n",
      "priclete\n",
      "brostynn\n",
      "sira\n",
      "pranc\n",
      "ary\n",
      "hinia\n",
      "maryana\n",
      "janee\n",
      "konnoy\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate_name(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d422d-07b9-4556-99b6-b2ee6b64ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
