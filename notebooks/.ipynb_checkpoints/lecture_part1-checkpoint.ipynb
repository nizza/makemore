{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a751b0-f23b-40ff-a01d-58dc043aa825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0127e-4334-4095-a433-c589eb43d2f8",
   "metadata": {},
   "source": [
    "# Broadcasting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be1cfc-536c-47d0-8716-8aed9b06cee0",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e07fab-2739-490d-b26d-810241f12ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = torch.tensor([[1, 1, 1],\n",
    "                  [5, 3, 2],\n",
    "                  [10, 10, 80]\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbe6eac-447e-4487-b827-f7f77b311b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9157d7b6-ab28-41e8-b131-dc5df659f581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3,  10, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "wrong_norms = t_data.sum(axis=1)\n",
    "print(wrong_norms)\n",
    "print(wrong_norms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aaef0b8-1137-44df-83aa-8ccc074ee0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.1000, 0.0100],\n",
       "        [1.6667, 0.3000, 0.0200],\n",
       "        [3.3333, 1.0000, 0.8000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data / wrong_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc61f31-e06d-4aaa-affc-d6830637e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3],\n",
      "        [ 10],\n",
      "        [100]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "correct_norms = t_data.sum(axis=1, keepdim=True)\n",
    "print(correct_norms)\n",
    "print(correct_norms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd470e2-55d1-4154-9974-c881ef60bf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.3333, 0.3333],\n",
       "        [0.5000, 0.3000, 0.2000],\n",
       "        [0.1000, 0.1000, 0.8000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data / correct_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d43c2-63a3-4734-8f52-adb583000b76",
   "metadata": {},
   "source": [
    "## numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64f582d-fc5f-431d-a29f-366ccedfac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = np.array([[1, 1, 1],\n",
    "                  [5, 3, 2],\n",
    "                  [10, 10, 80]\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1701b17d-1eeb-4da6-b6ae-62e7b3c18f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce70837d-9afb-4649-8451-998e500269f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3  10 100]\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "wrong_norms = n_data.sum(axis=1,)\n",
    "print(wrong_norms)\n",
    "print(wrong_norms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60bacec-c42f-4172-9c06-faa333744f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.1       , 0.01      ],\n",
       "       [1.66666667, 0.3       , 0.02      ],\n",
       "       [3.33333333, 1.        , 0.8       ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data / wrong_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4ff8f6-9c6d-4231-8c6b-2a5e48d54d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3]\n",
      " [ 10]\n",
      " [100]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "correct_norms = n_data.sum(axis=1, keepdims=True)\n",
    "print(correct_norms)\n",
    "print(correct_norms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e30875bc-4905-456c-bfdb-08b5f5490c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.5       , 0.3       , 0.2       ],\n",
       "       [0.1       , 0.1       , 0.8       ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data / correct_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794140fc-1704-481b-b2bf-9bbc1d0c32b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39506b0f-8b82-4ee7-9e55-d910df524f43",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ab702b1-4a1c-4a6a-affa-e35fd4666948",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[4., 5, 6],\n",
    "                  [1, 2, 3]\n",
    "                 ], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ba6773-7cc7-4998-904d-0acede5e9faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef396f7e-a092-4303-9358-25964e0310b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[3., 3]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ef7536b-cce5-4f8c-829e-2b050c91396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 21., 27.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = x @ W\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a1401ba-eda6-4e0d-a14c-56d11ae7e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor([[5, 1, 7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77bf75fb-06e2-43ab-84dc-08bb2672ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ((y_pred - y_true)**2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a20f49a-effd-4bef-9576-97abfc594f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(900., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73db1413-4e62-43f3-ba93-88e341813877",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de521481-3af5-4fea-98be-b81f33deabaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60., 120., 120.],\n",
       "        [ 60., 120., 120.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b57375-dcd3-494e-bb79-41dd7d82f4f0",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088f1e7-c7ec-4749-a6f9-d88f0170ff73",
   "metadata": {},
   "source": [
    "## Model with frequency counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a90c498-9880-4257-bdf1-ccf3e71fa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2940c8fc-ddbd-496a-a817-58778c7c0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the names from the file\n",
    "names = open(file_path, 'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60a339bd-5018-43c2-8609-5674c1a31557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0b88b92-cb70-4011-858e-3ac238028cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:5][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1549df66-2578-4be8-84ae-c058596ecf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bigrams dataset\n",
    "X = []\n",
    "y = []\n",
    "for name in names:\n",
    "\n",
    "    # adding start and end characters\n",
    "    word = '.' + name + '.'\n",
    "\n",
    "    # creating all bigrams for the current word\n",
    "    for ch1, ch2 in zip(word[:-1] ,word[1:]):\n",
    "\n",
    "        X.append(ch1)\n",
    "        y.append(ch2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f7cb4fe-2d75-4cd9-bfcf-3b009e6a53aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating vocabulary of all possible characters\n",
    "vocabulary = sorted(list(set(X)))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63f62c95-e307-4044-9d86-7155895357a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionaries to conver from character to numerical index, and viceversa\n",
    "stoi = {v:i for i,v in enumerate(vocabulary)}\n",
    "itos = {i:v for i,v in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b163d30e-ca65-4cf1-be9f-475b55a5dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a matrix computing the frequency of each bigram\n",
    "\n",
    "# initializing the counter matrix\n",
    "C = torch.zeros((len(vocabulary), len(vocabulary)))\n",
    "\n",
    "# going through all the bigrams\n",
    "for x_i, y_i in zip(X, y):\n",
    "\n",
    "    # getting the numerical indexes for the current characters\n",
    "    x_i_idx = stoi[x_i]\n",
    "    y_i_idx = stoi[y_i]\n",
    "\n",
    "    # updating the frequency matrix\n",
    "    C[x_i_idx, y_i_idx] += 1\n",
    "\n",
    "# Smoothing the frequency matrix to avoid any zeros\n",
    "C +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc26b863-deee-497e-bf79-1fdb7224ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the frequency matrix so that every row is a probability distribution\n",
    "row_sums = C.sum(axis=1, keepdim=True)\n",
    "C = C / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b0fffa6-e0c9-4ca4-8d21-a19bae0fc3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking that each row sums to 1\n",
    "C.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccf3e139-2506-4151-b1e7-6efb25709ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zarr\n",
      "awigeinnemt\n",
      "ahiahaki\n",
      "chrairar\n",
      "aikemian\n",
      "ckait\n",
      "tymiahas\n",
      "ka\n",
      "mi\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# Generating names from the learnt distribution\n",
    "\n",
    "def generate_name(M):\n",
    "    \"\"\"\n",
    "    Generates a name, given a bigram probability distribution matrix (M)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # starting with the word delimiter character\n",
    "    ch = '.'\n",
    "    generated = []\n",
    "    \n",
    "    while True:\n",
    "        # converting to index \n",
    "        idx = stoi[ch]\n",
    "        \n",
    "        # getting the corresponding row from the frequency matrix\n",
    "        dist = M[idx]\n",
    "        \n",
    "        # getting a sample from the probability distribution\n",
    "        idx = torch.multinomial(dist, 1, replacement=True).item()\n",
    "        \n",
    "        # converting index to character\n",
    "        ch = itos[idx]\n",
    "        \n",
    "        # stopping when delimiter char is reached\n",
    "        if ch == '.':\n",
    "            break\n",
    "        else:\n",
    "            generated.append(ch)\n",
    "    \n",
    "    return \"\".join(generated)\n",
    "\n",
    "for i in range(10):\n",
    "    print(generate_name(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f7219-db38-487b-8037-5a7be1922612",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "327e1532-1e4b-410f-8c69-e939129b2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "W = torch.rand((len(vocabulary), len(vocabulary)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d218cd0-6300-443a-8281-4fa181996579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0005, grad_fn=<MinBackward1>),\n",
       " tensor(0.9994, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.min(), W.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8a3ca83-5b73-4598-960b-dbf84f82d9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3329, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test:\n",
    "# Computing the average loss function, one sample at a time \n",
    "\n",
    "log_loss = 0\n",
    "for x_i, y_i in zip(X,y):\n",
    "    #print(x_i, y_i)\n",
    "\n",
    "    # converting characters to index\n",
    "    x_i_index = stoi[x_i]\n",
    "    y_i_index = stoi[y_i]\n",
    "\n",
    "    # converting index to one hot encoding vector\n",
    "    input_ =  torch.nn.functional.one_hot(torch.tensor(x_i_index), len(vocabulary)).float()\n",
    "\n",
    "    # multiplying ohe vector with weights matrix to select the row corresponding to the current character\n",
    "    row = input_ @ W\n",
    "\n",
    "    # normalizing the row to get a probability distribution (softmax)\n",
    "    row = torch.exp(row) \n",
    "    row /= row.sum()\n",
    "\n",
    "    # getting the predicted probability for next character in the training set\n",
    "    pred_p = row[y_i_index]\n",
    "\n",
    "    # updating log_loss accumulator\n",
    "    log_loss += torch.log(pred_p)\n",
    "\n",
    "# computing the average negative log_loss\n",
    "avg_neg_log_loss = - log_loss / len(X)\n",
    "avg_neg_log_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1e00340-45b6-4f42-bc7a-1f0918b29b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146]), torch.Size([228146]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the datasets to numerical tensors\n",
    "Xt = torch.tensor([stoi[x_i] for x_i in X])\n",
    "yt = torch.tensor([stoi[y_i] for y_i in y])\n",
    "Xt.shape, yt.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fd6a7ed-c512-47c7-b0a3-5d9b97e0bfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 27]), torch.Size([228146, 27]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying One Hot encoding to the numerical tensors\n",
    "Xoh = torch.nn.functional.one_hot(Xt, len(vocabulary)).float()\n",
    "yoh = torch.nn.functional.one_hot(yt, len(vocabulary)).float()\n",
    "Xoh.shape, yoh.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3eac279-a040-44a5-b0d0-41f83a9caeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing each row sto get a probability distribution (softmax)\n",
    "P = torch.exp(W) /  torch.exp(W).sum(axis=1, keepdims=True)\n",
    "P.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1b58ef9-8802-472e-a8df-3810c7dffe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the log loss in a vectorized way\n",
    "\n",
    "# Multiplying the input OHE matrix by P, so that every sample of X selects the appropriate row from P\n",
    "# (the one corresponding to the first character of the bigram)\n",
    "rows = Xoh @ P\n",
    "\n",
    "# Selecting for each row the appropriate element\n",
    "# (the one corresponding to the second character of the bigram)\n",
    "output = rows[torch.arange(len(yt)), yt ]\n",
    "\n",
    "# computing the average, negative log loss\n",
    "anll =  - output.log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "221f35e9-ad47-4c3b-a909-4c3079f6847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3329, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51586213-9d6b-4d1a-99e3-37b053adcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass to compute gradients\n",
    "W.grad = None\n",
    "anll.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c13c878-cd7f-4ed7-8f0b-7be56f402043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the weights using the gradient \n",
    "W = W -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ce8f3cd-f472-4c70-9e4a-3b77cbc4bccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3561, grad_fn=<NegBackward0>)\n",
      "tensor(3.3388, grad_fn=<NegBackward0>)\n",
      "tensor(3.3218, grad_fn=<NegBackward0>)\n",
      "tensor(3.3051, grad_fn=<NegBackward0>)\n",
      "tensor(3.2888, grad_fn=<NegBackward0>)\n",
      "tensor(3.2729, grad_fn=<NegBackward0>)\n",
      "tensor(3.2572, grad_fn=<NegBackward0>)\n",
      "tensor(3.2419, grad_fn=<NegBackward0>)\n",
      "tensor(3.2270, grad_fn=<NegBackward0>)\n",
      "tensor(3.2124, grad_fn=<NegBackward0>)\n",
      "tensor(3.1981, grad_fn=<NegBackward0>)\n",
      "tensor(3.1842, grad_fn=<NegBackward0>)\n",
      "tensor(3.1706, grad_fn=<NegBackward0>)\n",
      "tensor(3.1573, grad_fn=<NegBackward0>)\n",
      "tensor(3.1444, grad_fn=<NegBackward0>)\n",
      "tensor(3.1318, grad_fn=<NegBackward0>)\n",
      "tensor(3.1196, grad_fn=<NegBackward0>)\n",
      "tensor(3.1077, grad_fn=<NegBackward0>)\n",
      "tensor(3.0960, grad_fn=<NegBackward0>)\n",
      "tensor(3.0847, grad_fn=<NegBackward0>)\n",
      "tensor(3.0738, grad_fn=<NegBackward0>)\n",
      "tensor(3.0631, grad_fn=<NegBackward0>)\n",
      "tensor(3.0527, grad_fn=<NegBackward0>)\n",
      "tensor(3.0426, grad_fn=<NegBackward0>)\n",
      "tensor(3.0328, grad_fn=<NegBackward0>)\n",
      "tensor(3.0232, grad_fn=<NegBackward0>)\n",
      "tensor(3.0139, grad_fn=<NegBackward0>)\n",
      "tensor(3.0049, grad_fn=<NegBackward0>)\n",
      "tensor(2.9961, grad_fn=<NegBackward0>)\n",
      "tensor(2.9876, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Performing several trining steps\n",
    "\n",
    "# Initializing model\n",
    "W = torch.rand((len(vocabulary), len(vocabulary)), requires_grad=True)\n",
    "\n",
    "steps = 30\n",
    "\n",
    "for i in range(steps):\n",
    "\n",
    "    ######################################################\n",
    "    # Applying One Hot encoding to the numerical tensors #\n",
    "    ######################################################\n",
    "    Xoh = torch.nn.functional.one_hot(Xt, len(vocabulary)).float()\n",
    "    \n",
    "    ##############################################\n",
    "    # Computing the log loss in a vectorized way #\n",
    "    ##############################################\n",
    "\n",
    "    # Multiplying the input OHE matrix by W, so that every sample of X selects the appropriate row from W\n",
    "    # (the one corresponding to the first character of the bigram)\n",
    "    rows = Xoh @ W\n",
    "\n",
    "    # normalizing each row sto get a probability distribution (softmax) \n",
    "    P = torch.exp(rows) /  torch.exp(rows).sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Selecting for each row the appropriate element\n",
    "    # (the one corresponding to the second character of the bigram)\n",
    "    output = P[torch.arange(len(yt)), yt ]\n",
    "    \n",
    "    # computing the average, negative log loss\n",
    "    anll =  - output.log().mean()\n",
    "    print(anll)\n",
    "\n",
    "    ##############################################\n",
    "    #    backward pass to compute gradients      #\n",
    "    ##############################################\n",
    "    W.grad = None\n",
    "    anll.backward()\n",
    "\n",
    "    ##############################################\n",
    "    #   updating the weights using the gradient  #\n",
    "    ##############################################\n",
    "    W.data += -(3 * W.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1c6fc-61c0-407b-be36-7a2541038cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
